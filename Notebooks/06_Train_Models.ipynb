{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "264b0a98-c50c-4cc4-85ff-acbbf2baaa96",
   "metadata": {},
   "source": [
    "# 06 â€” Train Models (Per Make & Body Type)\n",
    "\n",
    "In this notebook we train a **separate model per (Make, Body_Type) group**.\n",
    "\n",
    "**Modelling approach:**\n",
    "- Time-based split:\n",
    "  - 80% Train\n",
    "  - 10% Validation\n",
    "  - 10% Test\n",
    "- Features:\n",
    "  - Year\n",
    "  - Month\n",
    "  - Month_Since_Start\n",
    "  - Make (one-hot)\n",
    "  - Body_Type (one-hot)\n",
    "- Model:\n",
    "  - `GradientBoostingRegressor`\n",
    "  - Small hyperparameter grid, chosen by validation MAE\n",
    "- Outputs:\n",
    "  - `final_predictions_detailed.csv` (row-level actual vs predicted by split)\n",
    "  - `model_scoring_summary.csv` (MAE / RMSE per segment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80e9744-9793-48e3-8c25-b2e63f76ae0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "PRED_PATH = os.path.join(DATA_DIR, \"final_predictions_detailed.csv\")\n",
    "METRICS_PATH = os.path.join(DATA_DIR, \"model_scoring_summary.csv\")\n",
    "\n",
    "df = pd.read_csv(os.path.join(DATA_DIR, \"clean_sales_with_features.csv\"))\n",
    "df[\"Year_Month\"] = pd.to_datetime(df[\"Year_Month\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f460f73-4f3d-46d1-a58c-17ec8398e75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build group dictionary\n",
    "groups = {}\n",
    "for (make, body), grp in df.groupby([\"Make\", \"Body_Type\"]):\n",
    "    if grp[\"Year_Month\"].nunique() < 16:\n",
    "        continue\n",
    "    groups[(make, body)] = grp.sort_values(\"Year_Month\").reset_index(drop=True)\n",
    "\n",
    "print(f\"Total groups for modelling: {len(groups)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03aa896-2ba9-4896-aac9-113838228ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_list = []\n",
    "metrics = []\n",
    "\n",
    "for (make, body), grp in groups.items():\n",
    "    months = grp[\"Year_Month\"].unique()\n",
    "    n = len(months)\n",
    "    train_end = int(n * 0.8)\n",
    "    val_end = int(n * 0.9)\n",
    "\n",
    "    train_m = months[:train_end]\n",
    "    val_m   = months[train_end:val_end]\n",
    "    test_m  = months[val_end:]\n",
    "\n",
    "    train_df = grp[grp[\"Year_Month\"].isin(train_m)].copy()\n",
    "    val_df   = grp[grp[\"Year_Month\"].isin(val_m)].copy()\n",
    "    test_df  = grp[grp[\"Year_Month\"].isin(test_m)].copy()\n",
    "\n",
    "    if len(train_df) == 0 or len(val_df) == 0 or len(test_df) == 0:\n",
    "        continue\n",
    "\n",
    "    feature_cols = [\"Year\", \"Month\", \"Month_Since_Start\", \"Make\", \"Body_Type\"]\n",
    "    target_col = \"Units_Sold\"\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), [\"Make\", \"Body_Type\"]),\n",
    "            (\"num\", \"passthrough\", [\"Year\", \"Month\", \"Month_Since_Start\"])\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    param_grid = [\n",
    "        {\"n_estimators\": 100, \"learning_rate\": 0.1,  \"max_depth\": 2},\n",
    "        {\"n_estimators\": 200, \"learning_rate\": 0.05, \"max_depth\": 3},\n",
    "        {\"n_estimators\": 300, \"learning_rate\": 0.05, \"max_depth\": 4},\n",
    "        {\"n_estimators\": 400, \"learning_rate\": 0.03, \"max_depth\": 3},\n",
    "    ]\n",
    "\n",
    "    best_model = None\n",
    "    best_mae = np.inf\n",
    "\n",
    "    X_train = train_df[feature_cols]\n",
    "    y_train = train_df[target_col]\n",
    "    X_val   = val_df[feature_cols]\n",
    "    y_val   = val_df[target_col]\n",
    "\n",
    "    for params in param_grid:\n",
    "        model = GradientBoostingRegressor(**params)\n",
    "        pipeline = Pipeline([(\"prep\", preprocessor), (\"model\", model)])\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        preds_val = pipeline.predict(X_val)\n",
    "        mae = mean_absolute_error(y_val, preds_val)\n",
    "        if mae < best_mae:\n",
    "            best_mae = mae\n",
    "            best_model = pipeline\n",
    "\n",
    "    if best_model is None:\n",
    "        continue\n",
    "\n",
    "    # Save train/val/test predictions\n",
    "    for split_name, split_df in [(\"Train\", train_df), (\"Validation\", val_df), (\"Test\", test_df)]:\n",
    "        temp = split_df.copy()\n",
    "        temp[\"Split\"] = split_name\n",
    "        temp[\"Predicted_Units_Sold\"] = best_model.predict(temp[feature_cols])\n",
    "        temp[\"Make\"] = make\n",
    "        temp[\"Body_Type\"] = body\n",
    "        predictions_list.append(temp)\n",
    "\n",
    "    # Metrics on validation + test combined\n",
    "    vt = pd.concat([val_df, test_df])\n",
    "    if len(vt) > 0:\n",
    "        y_true = vt[target_col]\n",
    "        y_pred = best_model.predict(vt[feature_cols])\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        rmse = math.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        metrics.append({\n",
    "            \"Make\": make,\n",
    "            \"Body_Type\": body,\n",
    "            \"MAE\": mae,\n",
    "            \"RMSE\": rmse,\n",
    "            \"N\": len(vt)\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4310c96e-c08d-45eb-af2f-7ed471cbd991",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions = (\n",
    "    pd.concat(predictions_list, ignore_index=True)\n",
    "    .sort_values([\"Make\", \"Body_Type\", \"Year_Month\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "final_predictions.to_csv(PRED_PATH, index=False)\n",
    "print(f\"Saved detailed predictions to: {PRED_PATH}\")\n",
    "\n",
    "metrics_df = (\n",
    "    pd.DataFrame(metrics)\n",
    "    .sort_values(\"MAE\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "metrics_df.to_csv(METRICS_PATH, index=False)\n",
    "\n",
    "print(f\"Saved metrics to: {METRICS_PATH}\")\n",
    "display(metrics_df.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
