{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97340839-c59f-4069-8cd7-03aade05ef3a",
   "metadata": {},
   "source": [
    "# Full Forecasting Pipeline  \n",
    "### SQL-Driven ETL → Feature Engineering → Model Training → Evaluation → Visualisation → SQL Insights\n",
    "\n",
    "This notebook combines the entire end-to-end forecasting workflow, including:\n",
    "\n",
    "1. **Load raw CSVs**\n",
    "2. **Insert raw + translation data into SQLite**\n",
    "3. **Run full SQL ETL cleaning pipeline**\n",
    "4. **Load cleaned dataset**\n",
    "5. **Feature engineering for time-series modelling**\n",
    "6. **Train Gradient Boosting models per (Make, Body_Type)**\n",
    "7. **Compute MAE/RMSE scoring**\n",
    "8. **Generate plots**\n",
    "9. **Run SQL analytics on cleaned table**\n",
    "\n",
    "This mirrors the complete working Python pipeline used in the project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df01659-d02f-4530-8ef5-70ae1c4c2c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb24e81-f306-4f8b-8483-4e5b389ffaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"Data\"\n",
    "\n",
    "RAW_CSV_PATH = os.path.join(DATA_DIR, \"raw_sales.csv\")\n",
    "TRANSLATION_CSV_PATH = os.path.join(DATA_DIR, \"translations.csv\")\n",
    "SQL_DB_PATH = os.path.join(DATA_DIR, \"auto_sales.db\")\n",
    "\n",
    "PLOT_DIR = os.path.join(DATA_DIR, \"plots\")\n",
    "FINAL_PRED_CSV = os.path.join(DATA_DIR, \"final_predictions_detailed.csv\")\n",
    "SCORING_CSV = os.path.join(DATA_DIR, \"model_scoring_summary.csv\")\n",
    "CLEANED_CSV = os.path.join(DATA_DIR, \"clean_sales_final.csv\")\n",
    "\n",
    "os.makedirs(PLOT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa1bd11-d902-48b5-9146-cd30c3a564f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(path):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "    try:\n",
    "        return pd.read_csv(path)\n",
    "    except UnicodeDecodeError:\n",
    "        return pd.read_csv(path, encoding=\"latin1\")\n",
    "    except pd.errors.ParserError:\n",
    "        return pd.read_csv(path, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33f6898-c2ad-4d09-998a-8aedd6fa3e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(path):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "    try:\n",
    "        return pd.read_csv(path)\n",
    "    except UnicodeDecodeError:\n",
    "        return pd.read_csv(path, encoding=\"latin1\")\n",
    "    except pd.errors.ParserError:\n",
    "        return pd.read_csv(path, sep=\";\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcb00ba-af03-4610-87dd-1d218694084d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(SQL_DB_PATH)\n",
    "raw_df.to_sql(\"raw_sales\", conn, if_exists=\"replace\", index=False)\n",
    "translations_df.to_sql(\"translations\", conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "with conn:\n",
    "    conn.execute(\"CREATE INDEX IF NOT EXISTS idx_raw_make ON raw_sales(Make);\")\n",
    "    conn.execute(\"CREATE INDEX IF NOT EXISTS idx_raw_body ON raw_sales(Body_Type);\")\n",
    "    conn.execute(\"CREATE INDEX IF NOT EXISTS idx_raw_yearmonth ON raw_sales(Year_Month);\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d770ed-7bbe-4ece-b93b-093c295abcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning_sql = \"\"\"\n",
    "-- SQL ETL pipeline (normalisation, cleaning, dedupe, casting, canonical date formatting)\n",
    "\n",
    "DROP TABLE IF EXISTS makes;\n",
    "CREATE TABLE makes AS\n",
    "SELECT DISTINCT COALESCE(t.Make_English, TRIM(rs.Make)) AS Make_Normalised\n",
    "FROM raw_sales rs\n",
    "LEFT JOIN translations t ON TRIM(rs.Make) = TRIM(t.Make);\n",
    "\n",
    "DROP TABLE IF EXISTS makes_id;\n",
    "CREATE TABLE makes_id AS\n",
    "SELECT ROW_NUMBER() OVER (ORDER BY Make_Normalised) AS MakeID,\n",
    "       Make_Normalised\n",
    "FROM makes;\n",
    "\n",
    "DROP TABLE IF EXISTS raw_parsed;\n",
    "CREATE TABLE raw_parsed AS\n",
    "SELECT *,\n",
    "    TRIM(Make) AS Make_raw,\n",
    "    TRIM(Body_Type) AS Body_Type_raw,\n",
    "    Year_Month AS Year_Month_raw,\n",
    "    CASE\n",
    "      WHEN LENGTH(Year_Month) >= 7 AND substr(Year_Month,5,1) IN ('-','/') THEN replace(Year_Month, '/', '-')\n",
    "      ELSE NULL\n",
    "    END AS Year_Month_iso\n",
    "FROM raw_sales;\n",
    "\n",
    "DROP TABLE IF EXISTS raw_filtered;\n",
    "CREATE TABLE raw_filtered AS\n",
    "SELECT *\n",
    "FROM raw_parsed\n",
    "WHERE COALESCE(Make_raw, '') <> ''\n",
    "  AND COALESCE(Body_Type_raw, '') <> ''\n",
    "  AND Units_Sold IS NOT NULL;\n",
    "\n",
    "DROP TABLE IF EXISTS raw_numeric;\n",
    "CREATE TABLE raw_numeric AS\n",
    "SELECT *,\n",
    "       CASE WHEN TRIM(Units_Sold) = '' THEN NULL\n",
    "            ELSE CAST(REPLACE(Units_Sold, ',', '') AS INTEGER)\n",
    "       END AS Units_Sold_n\n",
    "FROM raw_filtered;\n",
    "\n",
    "DROP TABLE IF EXISTS raw_positive;\n",
    "CREATE TABLE raw_positive AS\n",
    "SELECT *\n",
    "FROM raw_numeric\n",
    "WHERE Units_Sold_n IS NOT NULL AND Units_Sold_n > 0;\n",
    "\n",
    "DROP TABLE IF EXISTS raw_dedup_prep;\n",
    "CREATE TABLE raw_dedup_prep AS\n",
    "SELECT *,\n",
    "   COALESCE(Year_Month_iso, Year_Month_raw) AS Year_Month_canonical\n",
    "FROM raw_positive;\n",
    "\n",
    "DROP TABLE IF EXISTS raw_dedup;\n",
    "CREATE TABLE raw_dedup AS\n",
    "SELECT *\n",
    "FROM (\n",
    "  SELECT *,\n",
    "         ROW_NUMBER() OVER (\n",
    "            PARTITION BY Make_raw, Body_Type_raw, Year_Month_canonical\n",
    "            ORDER BY Units_Sold_n DESC\n",
    "         ) AS rn\n",
    "  FROM raw_dedup_prep\n",
    ")\n",
    "WHERE rn = 1;\n",
    "\n",
    "DROP TABLE IF EXISTS raw_dates;\n",
    "CREATE TABLE raw_dates AS\n",
    "SELECT *,\n",
    "       CASE\n",
    "         WHEN LENGTH(Year_Month_canonical) >= 7\n",
    "         THEN substr(Year_Month_canonical,1,7) || '-01'\n",
    "         ELSE NULL\n",
    "       END AS Year_Month_clean\n",
    "FROM raw_dedup;\n",
    "\n",
    "DROP TABLE IF EXISTS raw_valid_dates;\n",
    "CREATE TABLE raw_valid_dates AS\n",
    "SELECT *\n",
    "FROM raw_dates\n",
    "WHERE Year_Month_clean IS NOT NULL;\n",
    "\n",
    "DROP TABLE IF EXISTS clean_sales;\n",
    "CREATE TABLE clean_sales AS\n",
    "SELECT rd.*, mi.MakeID, mi.Make_Normalised AS Make\n",
    "FROM raw_valid_dates rd\n",
    "LEFT JOIN makes_id mi\n",
    "  ON mi.Make_Normalised = COALESCE(\n",
    "       (SELECT Make_English FROM translations WHERE TRIM(translations.Make)=TRIM(rd.Make_raw)),\n",
    "       TRIM(rd.Make_raw)\n",
    "     );\n",
    "\n",
    "DROP TABLE IF EXISTS clean_sales_final;\n",
    "CREATE TABLE clean_sales_final AS\n",
    "SELECT\n",
    "    mi.MakeID,\n",
    "    mi.Make_Normalised AS Make,\n",
    "    TRIM(rd.Body_Type_raw) AS Body_Type,\n",
    "    date(rd.Year_Month_clean) AS Year_Month,\n",
    "    rd.Units_Sold_n AS Units_Sold\n",
    "FROM clean_sales rd\n",
    "LEFT JOIN makes_id mi ON mi.MakeID = rd.MakeID;\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "with conn:\n",
    "    conn.executescript(cleaning_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84be303c-1134-4f69-958d-f0258018423c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = pd.read_sql_query(\n",
    "    \"SELECT * FROM clean_sales_final ORDER BY Make, Body_Type, Year_Month;\", conn\n",
    ")\n",
    "clean_df[\"Year_Month\"] = pd.to_datetime(clean_df[\"Year_Month\"])\n",
    "clean_df.to_csv(CLEANED_CSV, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3736bb3e-b4a8-499b-b16b-86e668a02b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_features(df, ref_date=None):\n",
    "    df = df.copy()\n",
    "    if ref_date is None:\n",
    "        ref_date = df[\"Year_Month\"].min()\n",
    "    df[\"Year\"] = df[\"Year_Month\"].dt.year\n",
    "    df[\"Month\"] = df[\"Year_Month\"].dt.month\n",
    "    df[\"Month_Since_Start\"] = (\n",
    "        (df[\"Year_Month\"].dt.year - ref_date.year) * 12\n",
    "        + (df[\"Year_Month\"].dt.month - ref_date.month)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "groups = {}\n",
    "for (make, body), grp in clean_df.groupby([\"Make\", \"Body_Type\"]):\n",
    "    if grp[\"Year_Month\"].nunique() < 16:\n",
    "        continue\n",
    "    groups[(make, body)] = grp.sort_values(\"Year_Month\").reset_index(drop=True)\n",
    "\n",
    "len(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd62b33-ce5f-4db3-adc2-6e9e0911ac38",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "predictions_list = []\n",
    "\n",
    "for (make, body), grp in groups.items():\n",
    "    months = grp[\"Year_Month\"].unique()\n",
    "    n = len(months)\n",
    "    train_end = int(n * 0.8)\n",
    "    val_end = int(n * 0.9)\n",
    "    train_m = months[:train_end]\n",
    "    val_m = months[train_end:val_end]\n",
    "    test_m = months[val_end:]\n",
    "\n",
    "    train_df = grp[grp[\"Year_Month\"].isin(train_m)].copy()\n",
    "    val_df = grp[grp[\"Year_Month\"].isin(val_m)].copy()\n",
    "    test_df = grp[grp[\"Year_Month\"].isin(test_m)].copy()\n",
    "\n",
    "    ref_date = train_df[\"Year_Month\"].min()\n",
    "    train_df = add_time_features(train_df, ref_date)\n",
    "    val_df = add_time_features(val_df, ref_date)\n",
    "    test_df = add_time_features(test_df, ref_date)\n",
    "\n",
    "    feature_cols = [\"Year\", \"Month\", \"Month_Since_Start\", \"Make\", \"Body_Type\"]\n",
    "    target_col = \"Units_Sold\"\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        [\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False),\n",
    "             [\"Make\", \"Body_Type\"]),\n",
    "            (\"num\", \"passthrough\", [\"Year\", \"Month\", \"Month_Since_Start\"]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    param_grid = [\n",
    "        {\"n_estimators\": 100, \"learning_rate\": 0.1, \"max_depth\": 2},\n",
    "        {\"n_estimators\": 200, \"learning_rate\": 0.05, \"max_depth\": 3},\n",
    "        {\"n_estimators\": 300, \"learning_rate\": 0.05, \"max_depth\": 4},\n",
    "        {\"n_estimators\": 400, \"learning_rate\": 0.03, \"max_depth\": 3},\n",
    "    ]\n",
    "\n",
    "    best_pipeline = None\n",
    "    best_mae = np.inf\n",
    "\n",
    "    for params in param_grid:\n",
    "        model = GradientBoostingRegressor(**params)\n",
    "        pipeline = Pipeline([(\"prep\", preprocessor), (\"model\", model)])\n",
    "        try:\n",
    "            pipeline.fit(train_df[feature_cols], train_df[target_col])\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        preds = pipeline.predict(val_df[feature_cols])\n",
    "        mae = mean_absolute_error(val_df[target_col], preds)\n",
    "        if mae < best_mae:\n",
    "            best_mae = mae\n",
    "            best_pipeline = pipeline\n",
    "\n",
    "    if best_pipeline is None:\n",
    "        continue\n",
    "\n",
    "    models[(make, body)] = best_pipeline\n",
    "\n",
    "    for split_name, df_split in [\n",
    "        (\"Train\", train_df),\n",
    "        (\"Validation\", val_df),\n",
    "        (\"Test\", test_df),\n",
    "    ]:\n",
    "        df_split = df_split.copy()\n",
    "        df_split[\"Split\"] = split_name\n",
    "        df_split[\"Predicted_Units_Sold\"] = best_pipeline.predict(\n",
    "            df_split[feature_cols]\n",
    "        )\n",
    "        df_split[\"Make\"] = make\n",
    "        df_split[\"Body_Type\"] = body\n",
    "        predictions_list.append(df_split)\n",
    "\n",
    "final_predictions = (\n",
    "    pd.concat(predictions_list, ignore_index=True)\n",
    "    .sort_values([\"Make\", \"Body_Type\", \"Year_Month\"])\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9565e5f-cbea-476f-b8ce-fe09f0fbf8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions.to_csv(FINAL_PRED_CSV, index=False)\n",
    "final_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07650eb-2c54-4015-862e-51232dbcb8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "for (make, body), grp in final_predictions.groupby([\"Make\", \"Body_Type\"]):\n",
    "    val_test = grp[grp[\"Split\"].isin([\"Validation\", \"Test\"])].copy()\n",
    "    if val_test.empty:\n",
    "        continue\n",
    "    y_true = val_test[\"Units_Sold\"]\n",
    "    y_pred = val_test[\"Predicted_Units_Sold\"]\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = math.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    metrics.append({\"Make\": make, \"Body_Type\": body, \"MAE\": mae, \"RMSE\": rmse})\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics).sort_values(\"MAE\").reset_index(drop=True)\n",
    "metrics_df.to_csv(SCORING_CSV, index=False)\n",
    "metrics_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7017ba6-fca3-4049-881d-ac3aef2e15ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_groups = metrics_df.head(5)[[\"Make\", \"Body_Type\"]].values.tolist()\n",
    "\n",
    "for make, body in top_groups:\n",
    "    group_df = final_predictions[\n",
    "        (final_predictions[\"Make\"] == make) &\n",
    "        (final_predictions[\"Body_Type\"] == body)\n",
    "    ].copy()\n",
    "    if group_df.empty:\n",
    "        continue\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    ax.plot(group_df[\"Year_Month\"], group_df[\"Units_Sold\"], label=\"Actual\")\n",
    "    ax.plot(group_df[\"Year_Month\"], group_df[\"Predicted_Units_Sold\"],\n",
    "            linestyle=\"--\", label=\"Predicted\")\n",
    "\n",
    "    ax.set_title(f\"{make} — {body}\")\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.set_ylabel(\"Units Sold\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    safe_name = f\"{make}_{body}\".replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "    save_path = os.path.join(PLOT_DIR, f\"{safe_name}.png\")\n",
    "    fig.savefig(save_path, dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015eb003-04c4-4b7d-a3f4-845edd646dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = \"\"\"\n",
    "SELECT Make, SUM(Units_Sold) AS Total_Sales\n",
    "FROM clean_sales_final\n",
    "GROUP BY Make\n",
    "ORDER BY Total_Sales DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "pd.read_sql_query(q1, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f8a048-4f0b-4887-bae1-4a6becde8b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "q2 = \"\"\"\n",
    "SELECT strftime('%Y-%m', Year_Month) AS YearMonth,\n",
    "       SUM(Units_Sold) AS Total_Units\n",
    "FROM clean_sales_final\n",
    "GROUP BY YearMonth\n",
    "ORDER BY YearMonth\n",
    "\"\"\"\n",
    "pd.read_sql_query(q2, conn).head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daa6d25-9421-473f-8358-a87cd6eb4f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "q3 = \"\"\"\n",
    "WITH Monthly AS (\n",
    "  SELECT Make, strftime('%Y-%m', Year_Month) AS ym,\n",
    "         SUM(Units_Sold) AS monthly_units\n",
    "  FROM clean_sales_final\n",
    "  GROUP BY Make, ym\n",
    "),\n",
    "Growth AS (\n",
    "  SELECT Make, ym, monthly_units,\n",
    "         LAG(monthly_units) OVER (PARTITION BY Make ORDER BY ym) AS prev_units\n",
    "  FROM Monthly\n",
    ")\n",
    "SELECT Make, ym, monthly_units, prev_units,\n",
    "       ROUND(100.0 * (monthly_units - prev_units) / prev_units, 2) AS growth_pct\n",
    "FROM Growth\n",
    "WHERE prev_units IS NOT NULL\n",
    "ORDER BY growth_pct DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "pd.read_sql_query(q3, conn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
